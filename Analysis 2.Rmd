---
title: "Analysis 2: Joining Cholesterol with Crimes and Web Scraping Wikipedia"
author: "Ishaan Phadnis"
date:  "October 18, 2019"
output: html_document
---

#Instructions

**Overview:** For each question, show your R code that you used to answer each question in the provided chunks. When a written response is required, be sure to answer the entire question in complete sentences outside the code chunks. When figures are required, be sure to follow all requirements to receive full credit. Point values are assigned for every part of this analysis.

**Helpful:** Make sure you knit the document as you go through the assignment. Check all your results in the created HTML file.

**Submission:** Submit via an electronic document on Sakai. Must be submitted as an HTML file generated in RStudio. 

```{r setup, include=F}
options(scipen=999)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
```

#Introduction

Does high cholesterol lead to high crime rates? Probably not, but web scraping will definitely lead to lower crime rates. This data analysis assignment is separated into three parts which cover material from the lectures on tidy data, joins, and webscraping. In Part 1, you will demonstrate the basic concept of joins by connecting relational data involving a cholesterol study. For this segment, spreading and gathering will be utilized to create a single tidy dataset ready for analysis. In Part 2, we will join all 5 datasets from the lecture series on web scraping. Part 3 will require an understanding of web scraping to import a table found on Wikipedia directly into R. The following R code reads in all datasets required for this assignment.

```{r,message=F}
# Data for Part 1
CHOL1=read_csv("Cholesterol.csv")
CHOL2=read_csv("Cholesterol2.csv")

CHOL1
CHOL2

# Data for Part 2
VIOLENT=read_csv("FINAL_VIOLENT.csv")
ZIP=read_csv("FINAL_ZIP.csv")
STATE_ABBREV=read_csv("FINAL_STATE_ABBREV.csv")
CENSUS=read_csv("FINAL_CENSUS.csv")
S_VS_D=read_csv("FINAL_SAFE_VS_DANGEROUS.CSV")

```







#Assignment

## Part 1: Cholesterol Experiment

The data frame `CHOL1` contains experimental results from randomly assigning *18* people to one of two competing margarine brands "A" and "B". The cholesterol of these patients was measured before the once before using the margarine brand, once after *4* weeks with the margarine brand, and then again after *8* weeks with the margarine brand. Researchers want to see if there is benefit of these brands of margarine on reducing an individual's cholesterol and want to determine if there is a statistically significant difference between the two competing brands. 
 
###Q1 *(3 Points)*

Start by examing the tables `CHOL1` and `CHOL2` and answering the following questions with *Yes* or *No* responses.

Is the variable `ID` in `CHOL1` a primary key? 

Answer *(1 Point)*: (Yes/No) Yes

Is the variable, `Margarine` in `CHOL1` a primary key?

Answer *(1 Point)*: (Yes/No) No

Is the variable, `Brand` in `CHOL2` a primary key? 

Answer *(1 Point)*: (Yes/No) No

###Q2 *(2 Points)*

In a new data frame called `CHOL1a` based on `CHOL1`, rename the variables `After4weeks` and `After8weeks` to nonsynctactic variable names `4` and `8`, respectively. Use `names(CHOL1a)` to display this modification.

```{r}
#
CHOL1a = rename(CHOL1, '4' = After4weeks, '8' = After8weeks)
CHOL1a
names(CHOL1a)
```

###Q3 *(4 Points)*

Use the `gather()` function on `CHOL1a` to create a new numeric variable called `Week` that contains numeric values *4* or *8* and a new numeric variable called `Response` that contains the Cholesterol after the corresponding number of weeks. Create a new data frame called `CHOL1b` with these modifications and use `str(Chol1b)` to show that both variables have been created correctly and are indeed numeric (an integer variable is a specific type of numeric variable).

```{r}
#
CHOL1b = CHOL1a %>% gather(Week, Response, 3:4)
CHOL1b
str(CHOL1b)
```

###Q4 *(4 Points)*

Now working with `CHOL2`, we want to spread the variable `Statistic` across multiple columns. Do this in a new data frame called `CHOL2a` and use `print(CHOL2a)` to display the modified complete table.

```{r}
#
CHOL2a = CHOL2 %>% spread(key = Statistic, value = Value)
CHOL2a
print(CHOL2a)
```

###Q5 *(3 Points)*

Start by examing the tables `CHOL1b` and `CHOL2a` and answering the following questions with *Yes* or *No* responses.

Is the variable `ID` in `CHOL1b` a primary key? 

Answer *(1 Point)*: (Yes/No) No

Is the variable, `Margarine` in `CHOL1b` a primary key?

Answer *(1 Point)*: (Yes/No) No

Is the variable, `Brand` in `CHOL2a` a primary key? 

Answer *(1 Point)*: (Yes/No) Yes

###Q6 *(4 Points)*

Get the nutritional facts of the different margarine brands in `CHOL2a` into the experimental results found in `CHOL1b` using a join. Create a new data frame named `CHOL.COMBINED` and display the table using `head(CHOL.COMBINED)`. This final data frame should contain *36* observations and *10* variables.
 
```{r}
#
CHOL.COMBINED = left_join(CHOL1b, CHOL2a, by = c("Margarine" = "Brand"))
CHOL.COMBINED
head(CHOL.COMBINED)
```








## Part 2: Linking Important Information to 2015 Violent Crimes Data

In the web scraping lectures, we created *5* CSV files. In this section, we are going to merge all of that data into one object called `FINAL.VIOLENT`. Follow the steps carefully to ensure you obey all instructions or you are likely to become a victim of a violent crime. **This is a joke and not a threat. I will not murder you if you don't follow the steps carefully. That is what hitmen/hitwomen are for.**

###Q1 *(2 Points)*

The dataset `S_VS_D` contains a variable `CLASS` where "S=Safe" and "D=Dangerous" according to the article *[These Are the 2018 Safest and Most Dangerous States in the U.S](https://www.securitysales.com/fire-intrusion/2018-safest-most-dangerous-states-us/)* by [Steve Karantzoulidis](https://www.securitysales.com/author/stevek/). We seek to compare the violent crime statistics for states not in this list. Use a filtering join to create a new data frame called `VIOLENT2` that only contains violent crime statistics from the states not represented in the data frame `S_VS_D`.  Use `str(VIOLENT2)` to display the variables and the dimensions of `VIOLENT2`.


```{r}
#
VIOLENT2 = anti_join(VIOLENT, S_VS_D, by = c("State" = "STATE"))
VIOLENT2
str(VIOLENT2)
```

###Q2 *(4 Points)*

Start by creating a new data set called `VIOLENT3` based on `VIOLENT2` that fixes some problems in the variable `City`. Specifically, we would like to change "Louisville Metro", "Nashville Metropolitan", and "Washington" to "Louisville", "Nashville", and "Washington Dc", respectively. 

Next, create a new data frame named `VIOLENT4` that connects the population change and density measures from 2019 contained in `CENSUS` to the cities and states in `VIOLENT3`. Use `head(VIOLENT4)` to give a preview of the new merged dataset.

Finally, in a complete sentence, identify any location(s) (Cities and States) missing census information.

Code and Output *(2 Points)*:
```{r}
#
VIOLENT3 = VIOLENT2 %>% mutate(City = ifelse(City == "Louisville Metro", "Louisville", ifelse(City == "Nashville Metropolitan", "Nashville", ifelse(City == "Washington", "Washington Dc", City))))
CENSUS1 = CENSUS %>% rename(City = Name)
VIOLENT4 = left_join(VIOLENT3, CENSUS1)

sum(is.na(VIOLENT4))
View(VIOLENT4)
```

Answer *(2 Points)*: (Place Answer Here Using Complete Sentences) 
The only city that is missing information is Lexington, Kentucky. The reason why is because when the two datasets are merged, this results in a NA value for 'Lexington' under the 'Change' and 'Density' columns and the reason why is because the city is written as 'Lexington Fayette' in the 'CENSUS1' dataset, which is different from the 'VIOLENT4' dataset. 


###Q3 *(6 Points)*

Either ambitiously using one step or less-ambitiously using multiple steps add the longitude and latitude information provided in `ZIP` to the cities and states in `VIOLENT4`. You will need to use `STATE_ABBREV` data to link these two data frames. Your final data frame named `FINAL.VIOLENT` should contain all of the information in `VIOLENT4` along with the variables `lat` and `lon` from `ZIP`. There should be **no** state abbreviations in `FINAL.VIOLENT` since this information is redundant. Use `str(FINAL.VIOLENT)` to demonstrate that everything worked as planned out by your imperial commander Dr. Mario. 

In `FINAL.VIOLENT` identify what cities are missing latitude and longitude. Closely, inspect both the `ZIP` and `VIOLENT4` data frames. Report the location(s) missing geographical informationand explain in complete sentences why this happened. 

Finally, challenge yourself and attempt to fix this problem in a new data frame called `FINAL.VIOLENT.FIX`. Use a combination of `str()` and `filter()` to only display the data in `FINAL.VIOLENT.FIX` for the location(s) that `FINAL.VIOLENT` was missing latitude and longitude. Do this in the second code chunk below.

Code and Output *(4 Points)*:
```{r}
#
STATE_ABBREV1 = STATE_ABBREV %>% rename(Abbrev = state)
VIOLENT5 = left_join(VIOLENT4, STATE_ABBREV1)
ZIP1 = ZIP %>% rename(Abbrev = state, City = city)
VIOLENT5

VIOLENT6 = left_join(VIOLENT5, ZIP1)
VIOLENT6

FINAL.VIOLENT = VIOLENT6 %>% select(-Abbrev)
FINAL.VIOLENT
str(FINAL.VIOLENT)

is.na(FINAL.VIOLENT$lat)
is.na(FINAL.VIOLENT$lon)
```

Answer *(1 Points)*: The only city that is missing a value for 'lat' and 'lon' is 'Washington Dc' and the reason why is because in the 'ZIP' dataset that is used to create 'FINAL.VIOLENT', 'Washington Dc' does not exist and is written as just 'Washington' and 'District of Columbia' is written differently in the 'ZIP' dataset as well (as just 'DC'). Therefore, there are no 'lat' and 'lon' values for 'Washington Dc' in 'FINAL.VIOLENT', therefore resulting in 'NA' for these columns. 

Code and Output *(1 Point)*:
```{r}
#
FINAL.VIOLENT.FIX = FINAL.VIOLENT 
FINAL.VIOLENT.FIX[is.na(FINAL.VIOLENT.FIX)] = c(38.89562, -77.01932)

str(FINAL.VIOLENT.FIX %>% filter(lat == 38.89562 & lon == -77.01932))
```





## Part 3: Web Scraping a Table From Wikipedia

Wikipedia contains a rough estimate of a billion tables. Search through Wikipedia pages and identify an article, completely unrelated to crimes data, that contains an HTML table that has at least *5* rows and *3* columns. You will be required to web scrape the table into a data frame or tibble into R. This portion will require a minor knowledge of the `rvest` package. Utilize information from the web scraping lectures and tutorials to assist you with this.

###Q1 *(4 Points)*

What is the URL of the Wikipedia page you plan on webscraping (Knit the Document and Check the Hyperlink)? 

Answer *(2 Points)*: https://en.wikipedia.org/wiki/Kemba_Walker#Career_statistics

In *2* to *5* sentences, Identify and describe the specific table you plan on web scraping. State the variables in *1* of the sentences.

Answer *(2 Points)*: The table that I plan on web scraping is based around the career statistics of Kemba Walker, an NBA player. These career statistics are essentially a dataframe of what Kemba has averaged throughout his professional career in several key basketball statistics (or variables). These variables include Year, Team, Games Played, Games Started, Minutes per Game, Field Goal %, 3 Point %, Free Throw %, Rebounds per Game, Assists per Game, Steals per Game, Blocks per Game, and Points per Game. 

###Q2 *(4 Points)*

Utilize the functions `read_html()` and `html_table()` to web scrape the specific table you described above. Internet access will be required for these functions to work. Create an R data frame named `DATA` which contains the information from the Wikipedia table. All code should be contained in the R code chunk below. Finally, use the `print()` function to display the table to demonstrate that everything worked as planned. The variable names and the content should match the table on the Wikipedia page you chose exactly. You are not required to perform any cleaning of this data.

```{r}
#
URL.DATA = "https://en.wikipedia.org/wiki/Kemba_Walker#Career_statistics"
DATA = URL.DATA %>% read_html() %>% html_table(fill = T) %>% .[[4]]
print(DATA)
```


